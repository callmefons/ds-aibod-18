---
title: "Miyama City Electric Power Demand by Circuit : Random Forest Regression Model"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

<style type="text/css">
</style>

<nav>
<a href="index.html">Home</a> |
<a href="miyama_analysis.html">Data Analysis</a> |
<a href="miyama_linear_model.html">Linear Regression Model</a> |
<a href="miyama_tree_model.html">Random Forest Regression Model</a> |
</nav>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(TZ = "Asia/Tokyo")

## Read R code From a lib file
source('lib.R')

packages(ggmap)
packages(leaflet)
packages(htmltools)
packages(DT)

packages(ggplot2)
packages(dplyr)
packages(lubridate)
packages(stringr)
packages(Nippon)
packages(moments)
packages(cluster)
packages(randomForest)
packages(dendextend)
packages(reshape2)
packages(Hmisc)
packages(DMwR)
packages(caTools)
packages(caret)

library(ggmap)
library(leaflet)
library(htmltools)
library(DT)
library(DMwR)
library(car)
library(caTools)
library(caret)

library(ggplot2)
library(dplyr)
library(lubridate)
library(stringr)
library(Nippon)
library(moments)
library(cluster)
library(randomForest)
library(dendextend)
library(reshape2)
library(Hmisc)
JPFont <- "HiraginoSans-W3"
```


```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
# Reload df saved in the earlier function.
load("MiyamaData.Rdata")
# Compactly display the internal structure of an R object.
str(df.day)
```

### The Prediction model
This is an R program to apply Random Forest regression technique on electric power consumption prediction. Our goal is to create a model that predicts daily electric power consumption of different households based on historical electric data, location, day of week, and plan type.

* We selected the data for 86 households and 42 locations for 6 months from 2018-01-01 to 2018-06-30 and divided it into training and test sets. For each machine learning model, we trained the model with the train set for predicting power consumption and used the test set to verify the prediction model.

```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
# Create new variable that can be used for modeling
df.elec <- df.day %>% group_by(user_id, location, wday, plan_type) %>%
  dplyr::summarise(pw_mean = round(mean(power), 4),
                   pw_max = round(max(power), 4),
                   pw_min = round(min(power), 4),
                   pw_sd = round(sd(power), 4)) 

# Encoding categorical location data.
df.elec$location <- as.factor(df.elec$location)
df.elec$location = factor(df.elec$location,
                       levels = as.vector(unique(df.elec$location)),
                       labels = c(1, 2, 3,
                                  4, 5, 6, 7 , 8 , 9, 10, 11, 12,
                                  13, 14, 15, 16, 17, 18, 19, 20,
                                  21, 22, 23, 24 ,25 ,26 ,27, 28,
                                  29, 30 ,31 ,32 ,33 ,34 ,35 ,36 , 
                                  37 ,38 ,39 ,40 ,41 ,42))

# Encoding categorical plan_type data.
df.elec$plan_type = factor(df.elec$plan_type,
                       levels = as.vector(unique(df.elec$plan_type)),
                       labels = c(1, 2 ,3 ,4 ,5 ,6))

# Encoding categorical day of week.
df.elec$wday <- as.factor(df.elec$wday)

# Selecting predictor variables.
df.elec <- df.elec[,c("location", "wday", "plan_type", "pw_max", "pw_min", "pw_sd", "pw_mean")]

# Removing all the zeros.
df.elec<-df.elec[!(df.elec$pw_mean <1),]

# Splitting the dataset into the Training set and Test set.

# Splitting the dataset into the Training set and Test set.
set.seed(123)
split=0.80 # Define an 80%/20% train/test split of the dataset
trainIndex <- createDataPartition(df.elec$pw_mean, p=split, list=FALSE)
training_set <- df.elec[ trainIndex,]
test_set <- df.elec[-trainIndex,]
```

* Our response variable will continue to be mean of Electric but now we included location, wday, plan_type, max, min and sd of electric power consumption as our list of predictor variables.

### Build Random Forest Regression
In this section, we'll train a Random Forest Regression model for predicting households daily electric power consumption based on historical electric data, location, day of week, and plan type. Unlike linear models, random forests are able to capture non-linear interaction between the features and the target. Therefore, we will try to switch to a random forest model, and check which is better suited for our scenario.

In below result we will define the training control, we use repeatedcv method to divide our dataset into 10 folds cross-validation and repeat only 3 repeat times in order to slows down our process.

```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE, rows.print=5}
# Define the training control
fitControl <- trainControl(
    method='repeatedcv',  #repeat k-fold cross validation
    number=10, # number of folds
    repeats=3, # number of repeats
    savePredictions = 'final',  # saves predictions for optimal tuning parameter
    classProbs = T  # should class probabilities be returned
) 
```

In random forest model, we cannot pre-understand our result because our model are randomly processing. When tuning an algorithm, it is important to have a good understanding of our algorithm so that we know what affect the parameters have on the model we are creating. In this case study, we will stick to tuning two parameters, namely the mtry and the ntree parameters that have the following affect on our random forest model.

* mtry: The number of features to use to build each tree. By default, we know that the random forest will use sqrt(16), or four features per tree.
* ntree: This is the total number of trees in your final ensemble model.
```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE, rows.print=5}
# Train the model using rf
set.seed(100)
model_rf <- train(pw_mean ~ ., 
         data = training_set, 
         method = "rf", 
         tuneLength = 10,
         trControl = fitControl, 
         importance = TRUE)
save(model_rf, file = "model_rf.Rdata") # Save model
```

#### Model summary
```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE, rows.print=5}
load("model_rf.Rdata")
model_rf
plot(model_rf)
```

* We can see the smallest RMSE = 40.8% when mtry = 25.

#### Checking variable importance for RF
```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE, rows.print=5}
varimp <- varImp(object=model_rf)
# Plotting Varianle importance for Random Forest
ggplot(varimp) +
  geom_bar(stat="identity", fill = "#2879d0")
  theme(axis.text.y = element_text(size=6, hjust = 1),
        axis.title=element_text(size=10),
        legend.position="none")
```

#### Predicting the Test set results
Once the model is created, it can then be used to make predictions on new examples that were not seen in training (the test data). 
```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
y_pred = predict(model_rf, newdata = test_set)
```

#### Display actual and predict observations
```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE}
# Actual and predict observations
actuals_preds <- data.frame(cbind(actuals=test_set$pw_mean, predicteds=y_pred))  
datatable(actuals_preds, options = list(pageLength = 6))
```

#### Performance comparison of machine learning approaches 
Once weâ€™ve created the different models **[Linear regression model](miyama_linear_model.html) **, let's compare their performance.

In the first, we will stack the models together in a list and then compare the results qualitatively using the function resamples().
```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE, rows.print=5}
load("model_lm.Rdata")
model_list <- list(lm = model_lm, rf = model_rf)
res <- resamples(model_list)
summary(res)
```
* Above you can see that the lm model has the lower RMSE and lower R-squared (it is the better of the two models).

Alternatively, we want to quantitatively test if one model is better than another. 
```{r, eval=TRUE, echo=TRUE, warning=FALSE, message=FALSE, rows.print=5}
# Compare model performances using resample()
compare_models(model_lm, model_rf)
```
* In this case they are not statistically different.


***

Looking for other parts of this series?

* **[Data analysis](miyama_analysis.html) **
* **[Linear regression model](miyama_linear_model.html) **
* **[Random Forest Regression Model](miyama_tree_model.html) **